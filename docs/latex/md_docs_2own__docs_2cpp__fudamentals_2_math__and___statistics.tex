\chapter{How to create a matrix on stack}
\hypertarget{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics}{}\label{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics}\index{How to create a matrix on stack@{How to create a matrix on stack}}
\label{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics_autotoc_md127}%
\Hypertarget{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics_autotoc_md127}%



\begin{DoxyItemize}
\item All matrix elements combination
\item Matrix multiplication and matrix element wise operations
\item List libraries to deal with linear algebra and vectors, matrices in cpp
\item How about optimization libraries that has algorithms to maximize the likelihood for login model for example ?
\item Request
\item A basic implementation of Linear Regression using Eigen.
\item Maximum Likelihood Estimation
\item Optimization Techniques to Estimate The Maximum Likelihood Parameters
\end{DoxyItemize}

In C++, you can create a matrix (2D array) on the stack using fixed sizes. Here\textquotesingle{}s an example\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{preprocessor}{\#include\ <iostream>}}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{keywordtype}{int}\ \mbox{\hyperlink{_c_make_c_x_x_compiler_id_8cpp_a0ddf1224851353fc92bfbff6f499fa97}{main}}()\ \{}
\DoxyCodeLine{\ \ \ \ \textcolor{comment}{//\ Create\ a\ 3x3\ matrix}}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordtype}{int}\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_a7b24a3f2f56eb1244ae69dacb4fecb6f}{matrix}}[3][3]\ =\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \{1,\ 2,\ 3\},}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \{4,\ 5,\ 6\},}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \{7,\ 8,\ 9\}}
\DoxyCodeLine{\ \ \ \ \};}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \textcolor{comment}{//\ Print\ the\ matrix}}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ i\ =\ 0;\ i\ <\ 3;\ i++)\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ \mbox{\hyperlink{class_vector__2d}{j}}\ =\ 0;\ \mbox{\hyperlink{class_vector__2d}{j}}\ <\ 3;\ \mbox{\hyperlink{class_vector__2d}{j}}++)\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ std::cout\ <<\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_a7b24a3f2f56eb1244ae69dacb4fecb6f}{matrix}}[i][\mbox{\hyperlink{class_vector__2d}{j}}]\ <<\ \textcolor{stringliteral}{"{}\ "{}};}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \}}
\DoxyCodeLine{\ \ \ \ \ \ \ \ std::cout\ <<\ std::endl;}
\DoxyCodeLine{\ \ \ \ \}}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordflow}{return}\ 0;}
\DoxyCodeLine{\}}

\end{DoxyCode}


This will print\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{1\ 2\ 3}
\DoxyCodeLine{4\ 5\ 6}
\DoxyCodeLine{7\ 8\ 9}

\end{DoxyCode}


This approach works for small fixed-\/size matrices. If you\textquotesingle{}re not sure about the size at compile time, or if the matrix can be large, you may consider using the heap or utilizing data structures like {\ttfamily std\+::vector} for dynamic sizing.\hypertarget{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics_autotoc_md128}{}\doxysection{\texorpdfstring{All matrix elements combination}{All matrix elements combination}}\label{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics_autotoc_md128}
Like diagonal, lower tringle, upper triangle, off diagonal and all other variations dont skip anything

Certainly! Let\textquotesingle{}s go through each of the combinations of matrix elements you mentioned\+:

Assuming we have an (n \textbackslash{}times n) matrix named {\ttfamily matrix}\+:


\begin{DoxyEnumerate}
\item {\bfseries{Main Diagonal}}\+: Elements for which row index is equal to column index.


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ i\ =\ 0;\ i\ <\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_ac0a41baf8b02de6cb4211d7f15f4872e}{n}};\ i++)\ \{}
\DoxyCodeLine{\ \ \ \ std::cout\ <<\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_a7b24a3f2f56eb1244ae69dacb4fecb6f}{matrix}}[i][i]\ <<\ \textcolor{stringliteral}{"{}\ "{}};}
\DoxyCodeLine{\}}

\end{DoxyCode}

\item {\bfseries{Secondary Diagonal}} (or anti-\/diagonal)\+: Elements for which row index and column index sum up to (n-\/1).


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ i\ =\ 0;\ i\ <\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_ac0a41baf8b02de6cb4211d7f15f4872e}{n}};\ i++)\ \{}
\DoxyCodeLine{\ \ \ \ std::cout\ <<\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_a7b24a3f2f56eb1244ae69dacb4fecb6f}{matrix}}[i][\mbox{\hyperlink{_s_d_l__opengl__glext_8h_ac0a41baf8b02de6cb4211d7f15f4872e}{n}}\ -\/\ i\ -\/\ 1]\ <<\ \textcolor{stringliteral}{"{}\ "{}};}
\DoxyCodeLine{\}}

\end{DoxyCode}

\item {\bfseries{Upper Triangle}}\+: Elements for which row index is less than column index.


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ i\ =\ 0;\ i\ <\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_ac0a41baf8b02de6cb4211d7f15f4872e}{n}};\ i++)\ \{}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ \mbox{\hyperlink{class_vector__2d}{j}}\ =\ i\ +\ 1;\ \mbox{\hyperlink{class_vector__2d}{j}}\ <\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_ac0a41baf8b02de6cb4211d7f15f4872e}{n}};\ \mbox{\hyperlink{class_vector__2d}{j}}++)\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ std::cout\ <<\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_a7b24a3f2f56eb1244ae69dacb4fecb6f}{matrix}}[i][\mbox{\hyperlink{class_vector__2d}{j}}]\ <<\ \textcolor{stringliteral}{"{}\ "{}};}
\DoxyCodeLine{\ \ \ \ \}}
\DoxyCodeLine{\}}

\end{DoxyCode}

\item {\bfseries{Lower Triangle}}\+: Elements for which row index is greater than column index.


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ i\ =\ 0;\ i\ <\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_ac0a41baf8b02de6cb4211d7f15f4872e}{n}};\ i++)\ \{}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ \mbox{\hyperlink{class_vector__2d}{j}}\ =\ 0;\ \mbox{\hyperlink{class_vector__2d}{j}}\ <\ i;\ \mbox{\hyperlink{class_vector__2d}{j}}++)\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ std::cout\ <<\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_a7b24a3f2f56eb1244ae69dacb4fecb6f}{matrix}}[i][\mbox{\hyperlink{class_vector__2d}{j}}]\ <<\ \textcolor{stringliteral}{"{}\ "{}};}
\DoxyCodeLine{\ \ \ \ \}}
\DoxyCodeLine{\}}

\end{DoxyCode}

\item {\bfseries{Off Diagonal}}\+: Every element that is not on the main diagonal.


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ i\ =\ 0;\ i\ <\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_ac0a41baf8b02de6cb4211d7f15f4872e}{n}};\ i++)\ \{}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ \mbox{\hyperlink{class_vector__2d}{j}}\ =\ 0;\ \mbox{\hyperlink{class_vector__2d}{j}}\ <\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_ac0a41baf8b02de6cb4211d7f15f4872e}{n}};\ \mbox{\hyperlink{class_vector__2d}{j}}++)\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ (i\ !=\ \mbox{\hyperlink{class_vector__2d}{j}})\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ std::cout\ <<\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_a7b24a3f2f56eb1244ae69dacb4fecb6f}{matrix}}[i][\mbox{\hyperlink{class_vector__2d}{j}}]\ <<\ \textcolor{stringliteral}{"{}\ "{}};}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \}}
\DoxyCodeLine{\ \ \ \ \}}
\DoxyCodeLine{\}}

\end{DoxyCode}

\item {\bfseries{Strict Upper Triangle}}\+: Upper triangle excluding the main diagonal.


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ i\ =\ 0;\ i\ <\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_ac0a41baf8b02de6cb4211d7f15f4872e}{n}};\ i++)\ \{}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ \mbox{\hyperlink{class_vector__2d}{j}}\ =\ i\ +\ 1;\ \mbox{\hyperlink{class_vector__2d}{j}}\ <\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_ac0a41baf8b02de6cb4211d7f15f4872e}{n}};\ \mbox{\hyperlink{class_vector__2d}{j}}++)\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ std::cout\ <<\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_a7b24a3f2f56eb1244ae69dacb4fecb6f}{matrix}}[i][\mbox{\hyperlink{class_vector__2d}{j}}]\ <<\ \textcolor{stringliteral}{"{}\ "{}};}
\DoxyCodeLine{\ \ \ \ \}}
\DoxyCodeLine{\}}

\end{DoxyCode}

\item {\bfseries{Strict Lower Triangle}}\+: Lower triangle excluding the main diagonal. 
\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ i\ =\ 1;\ i\ <\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_ac0a41baf8b02de6cb4211d7f15f4872e}{n}};\ i++)\ \{}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ \mbox{\hyperlink{class_vector__2d}{j}}\ =\ 0;\ \mbox{\hyperlink{class_vector__2d}{j}}\ <\ i;\ \mbox{\hyperlink{class_vector__2d}{j}}++)\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ std::cout\ <<\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_a7b24a3f2f56eb1244ae69dacb4fecb6f}{matrix}}[i][\mbox{\hyperlink{class_vector__2d}{j}}]\ <<\ \textcolor{stringliteral}{"{}\ "{}};}
\DoxyCodeLine{\ \ \ \ \}}
\DoxyCodeLine{\}}

\end{DoxyCode}

\end{DoxyEnumerate}

All the above combinations are for square matrices (where number of rows equals number of columns). Some of these terms might differ slightly depending on context or literature, but these are the general definitions and implementations in C++.\hypertarget{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics_autotoc_md129}{}\doxysection{\texorpdfstring{Matrix multiplication and matrix element wise operations}{Matrix multiplication and matrix element wise operations}}\label{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics_autotoc_md129}
Sure, let\textquotesingle{}s walk through matrix multiplication and element-\/wise operations\+:


\begin{DoxyEnumerate}
\item {\bfseries{Matrix Multiplication}}\+: For two matrices {\ttfamily A} and {\ttfamily B} to be multiplied, the number of columns in {\ttfamily A} must be equal to the number of rows in {\ttfamily B}. The result of the multiplication is a matrix {\ttfamily C} with the number of rows of {\ttfamily A} and the number of columns of {\ttfamily B}.
\end{DoxyEnumerate}

For matrices\+: {\ttfamily A} of dimension (m \textbackslash{}times n) {\ttfamily B} of dimension (n \textbackslash{}times p) The resultant matrix {\ttfamily C} will be of dimension (m \textbackslash{}times p).


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{comment}{//\ Assuming\ A\ is\ of\ dimension\ m\ x\ n\ and\ B\ is\ of\ dimension\ n\ x\ p}}
\DoxyCodeLine{\textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ i\ =\ 0;\ i\ <\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_af593500c283bf1a787a6f947f503a5c2}{m}};\ i++)\ \{}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ \mbox{\hyperlink{class_vector__2d}{j}}\ =\ 0;\ \mbox{\hyperlink{class_vector__2d}{j}}\ <\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_aa5367c14d90f462230c2611b81b41d23}{p}};\ \mbox{\hyperlink{class_vector__2d}{j}}++)\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d}{C}}[i][\mbox{\hyperlink{class_vector__2d}{j}}]\ =\ 0;}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ \mbox{\hyperlink{class_vector__2d}{k}}\ =\ 0;\ \mbox{\hyperlink{class_vector__2d}{k}}\ <\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_ac0a41baf8b02de6cb4211d7f15f4872e}{n}};\ \mbox{\hyperlink{class_vector__2d}{k}}++)\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d}{C}}[i][\mbox{\hyperlink{class_vector__2d}{j}}]\ +=\ \mbox{\hyperlink{class_vector__2d}{A}}[i][\mbox{\hyperlink{class_vector__2d}{k}}]\ *\ \mbox{\hyperlink{class_vector__2d}{B}}[\mbox{\hyperlink{class_vector__2d}{k}}][\mbox{\hyperlink{class_vector__2d}{j}}];}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \}}
\DoxyCodeLine{\ \ \ \ \}}
\DoxyCodeLine{\}}

\end{DoxyCode}



\begin{DoxyEnumerate}
\item {\bfseries{Element-\/wise Operations}}\+:
\begin{DoxyItemize}
\item {\bfseries{Addition}}\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ i\ =\ 0;\ i\ <\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_af593500c283bf1a787a6f947f503a5c2}{m}};\ i++)\ \{}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ \mbox{\hyperlink{class_vector__2d}{j}}\ =\ 0;\ \mbox{\hyperlink{class_vector__2d}{j}}\ <\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_ac0a41baf8b02de6cb4211d7f15f4872e}{n}};\ \mbox{\hyperlink{class_vector__2d}{j}}++)\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d}{C}}[i][\mbox{\hyperlink{class_vector__2d}{j}}]\ =\ \mbox{\hyperlink{class_vector__2d}{A}}[i][\mbox{\hyperlink{class_vector__2d}{j}}]\ +\ \mbox{\hyperlink{class_vector__2d}{B}}[i][\mbox{\hyperlink{class_vector__2d}{j}}];}
\DoxyCodeLine{\ \ \ \ \}}
\DoxyCodeLine{\}}

\end{DoxyCode}

\item {\bfseries{Subtraction}}\+:

{\ttfamily cpp for (int i = 0; i \texorpdfstring{$<$}{<} m; i++) \{ for (int j = 0; j \texorpdfstring{$<$}{<} n; j++) \{ C\mbox{[}i\mbox{]}\mbox{[}j\mbox{]} = A\mbox{[}i\mbox{]}\mbox{[}j\mbox{]} -\/ B\mbox{[}i\mbox{]}\mbox{[}j\mbox{]}; \} \} }
\item {\bfseries{Multiplication}} (Element-\/wise/\+Hadamard product)\+:

{\ttfamily cpp for (int i = 0; i \texorpdfstring{$<$}{<} m; i++) \{ for (int j = 0; j \texorpdfstring{$<$}{<} n; j++) \{ C\mbox{[}i\mbox{]}\mbox{[}j\mbox{]} = A\mbox{[}i\mbox{]}\mbox{[}j\mbox{]} \texorpdfstring{$\ast$}{*} B\mbox{[}i\mbox{]}\mbox{[}j\mbox{]}; \} \} }
\item {\bfseries{Division}}\+: {\ttfamily cpp for (int i = 0; i \texorpdfstring{$<$}{<} m; i++) \{ for (int j = 0; j \texorpdfstring{$<$}{<} n; j++) \{ C\mbox{[}i\mbox{]}\mbox{[}j\mbox{]} = A\mbox{[}i\mbox{]}\mbox{[}j\mbox{]} / B\mbox{[}i\mbox{]}\mbox{[}j\mbox{]}; // Make sure to handle the case when B\mbox{[}i\mbox{]}\mbox{[}j\mbox{]} = 0 \} \} }
\end{DoxyItemize}
\end{DoxyEnumerate}

For the element-\/wise operations, matrices {\ttfamily A} and {\ttfamily B} should have the same dimensions (both (m \textbackslash{}times n)), otherwise the operations are not defined. The resultant matrix {\ttfamily C} will also be of the same dimension (m \textbackslash{}times n).

Note\+: Error handling (like checking for matrix dimensions compatibility) is omitted for simplicity. In a real-\/world application, you would need to add those checks before performing these operations.\hypertarget{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics_autotoc_md130}{}\doxysection{\texorpdfstring{List libraries to deal with linear algebra and vectors, matrices in cpp}{List libraries to deal with linear algebra and vectors, matrices in cpp}}\label{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics_autotoc_md130}
Here is a list of some popular C++ libraries used for dealing with linear algebra, vectors, and matrices\+:


\begin{DoxyEnumerate}
\item {\bfseries{Eigen}}\+: This is probably one of the most widely used C++ libraries for linear algebra. It provides various operations for manipulating matrices, vectors, and numerical solvers. It is both high-\/performance and easy to use.
\item {\bfseries{Armadillo}}\+: Armadillo is a high-\/quality C++ linear algebra library, aiming towards a good balance between speed and ease of use. The syntax is deliberately similar to Matlab.
\item {\bfseries{BLAS (Basic Linear Algebra Subprograms)}}\+: This is a standard library that provides efficient operations for many kinds of vector and matrix operations. Although it\textquotesingle{}s a low-\/level library and may be more complex to use, it is very fast due to hardware optimizations.
\item {\bfseries{LAPACK (Linear Algebra PACKage)}}\+: This is a standard library for performing more advanced linear algebra operations, such as decompositions and eigenvalue calculations. LAPACK interfaces well with BLAS for efficient computations.
\item {\bfseries{Boost u\+BLAS}}\+: This is part of the larger Boost library collection. It provides classes for vectors and matrices and operations for linear algebra.
\item {\bfseries{GSL (GNU Scientific Library)}}\+: It provides a wide range of mathematical routines such as random number generators, special functions, and least-\/squares fitting. It has a CBLAS interface which provides basic routines for vector and matrix operations.
\item {\bfseries{GLM (Open\+GL Mathematics)}}\+: This is a header only C++ mathematics library for graphics software based on the Open\+GL Shading Language (GLSL) specifications. It\textquotesingle{}s commonly used in computer graphics tasks.
\end{DoxyEnumerate}

Remember to choose the library that best fits your needs and requirements. Some are easier to use and have more features, while others are more minimal but may offer better performance.\hypertarget{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics_autotoc_md131}{}\doxysection{\texorpdfstring{How about optimization libraries that has algorithms to maximize the likelihood for login model for example ?}{How about optimization libraries that has algorithms to maximize the likelihood for login model for example ?}}\label{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics_autotoc_md131}
There are several libraries in C++ that can help you with optimization tasks such as maximizing the likelihood for a logistic model\+:


\begin{DoxyEnumerate}
\item {\bfseries{NLopt}}\+: NLopt is a library for nonlinear optimization, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms. It provides capabilities for global optimization, local optimization, and constrained optimization.
\item {\bfseries{Ceres Solver}}\+: Ceres Solver is an open-\/source C++ library for modeling and solving large, complicated optimization problems. It can be used to solve Non-\/linear Least Squares problems with bounds constraints and general unconstrained optimization problems.
\item {\bfseries{Optimization with Ipopt and Cpp\+AD}}\+: Ipopt (Interior Point OPTimizer) is a software package for large-\/scale nonlinear optimization. Cpp\+AD is a C++ library that computes derivatives. Together they provide a powerful tool for optimization.
\item {\bfseries{Dlib}}\+: Dlib is a modern C++ toolkit containing machine learning algorithms and tools for creating complex software in C++ to solve real-\/world problems. It provides a number of optimization algorithms, including for numerical optimization.
\item {\bfseries{GSL -\/ GNU Scientific Library}}\+: The library provides a broad set of mathematical routines such as random number generators, special functions, and least-\/squares fitting. The multi-\/dimensional minimization algorithms might be helpful for your purpose.
\end{DoxyEnumerate}

The choice of library depends heavily on the specifics of your problem and your specific use case.\hypertarget{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics_autotoc_md132}{}\doxysection{\texorpdfstring{Request}{Request}}\label{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics_autotoc_md132}
Give me linear regression class that has optimization for estimating the parameters assume computing the parameters numerically donâ€™t use other libraries use only CPP standard The estimation should has option either compute using least square method or using maximum likelihood method, both cases should the derivative computed numerically


\begin{DoxyItemize}
\item Implementing numerical methods for optimization, such as gradient descent, from scratch requires careful consideration due to the numerous pitfalls of numerical computation. Here\textquotesingle{}s a very basic and rough implementation of a Linear Regression class using least squares and gradient descent for Maximum Likelihood Estimation. Please note that these implementations are very rudimentary and should be enhanced for robustness and performance before use in any kind of production or research code.
\end{DoxyItemize}


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{preprocessor}{\#include\ <vector>}}
\DoxyCodeLine{\textcolor{preprocessor}{\#include\ <numeric>}}
\DoxyCodeLine{\textcolor{preprocessor}{\#include\ <cmath>}}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{keyword}{class\ }\mbox{\hyperlink{class_vector__2d}{LinearRegression}}\ \{}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordtype}{double}\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{slope}};}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordtype}{double}\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{intercept}};}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{keyword}{public}:}
\DoxyCodeLine{\ \ \ \ \textcolor{comment}{//\ Estimate\ parameters\ using\ least\ squares\ method}}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordtype}{void}\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{least\_squares}}(\textcolor{keyword}{const}\ std::vector<double>\&\ \mbox{\hyperlink{_s_d_l__opengl_8h_abe1577120f1766eff913e9a74879f373}{x}},\ \textcolor{keyword}{const}\ std::vector<double>\&\ \mbox{\hyperlink{_s_d_l__opengl_8h_a1675d9d7bb68e1657ff028643b4037e3}{y}})\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{keywordtype}{double}\ \mbox{\hyperlink{class_vector__2d}{x\_mean}}\ =\ std::accumulate(\mbox{\hyperlink{_s_d_l__opengl_8h_abe1577120f1766eff913e9a74879f373}{x}}.begin(),\ \mbox{\hyperlink{_s_d_l__opengl_8h_abe1577120f1766eff913e9a74879f373}{x}}.end(),\ 0.0)\ /\ \mbox{\hyperlink{_s_d_l__opengl_8h_abe1577120f1766eff913e9a74879f373}{x}}.size();}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{keywordtype}{double}\ \mbox{\hyperlink{class_vector__2d}{y\_mean}}\ =\ std::accumulate(\mbox{\hyperlink{_s_d_l__opengl_8h_a1675d9d7bb68e1657ff028643b4037e3}{y}}.begin(),\ \mbox{\hyperlink{_s_d_l__opengl_8h_a1675d9d7bb68e1657ff028643b4037e3}{y}}.end(),\ 0.0)\ /\ \mbox{\hyperlink{_s_d_l__opengl_8h_a1675d9d7bb68e1657ff028643b4037e3}{y}}.size();}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{keywordtype}{double}\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_abb50fc1ead3a02a46fb52daa2045f95c}{num}}\ =\ 0.0;}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{keywordtype}{double}\ \mbox{\hyperlink{class_vector__2d}{den}}\ =\ 0.0;}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{size\_t}\ i\ =\ 0;\ i\ <\ \mbox{\hyperlink{_s_d_l__opengl_8h_abe1577120f1766eff913e9a74879f373}{x}}.size();\ i++)\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_abb50fc1ead3a02a46fb52daa2045f95c}{num}}\ +=\ (\mbox{\hyperlink{_s_d_l__opengl_8h_abe1577120f1766eff913e9a74879f373}{x}}[i]\ -\/\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{x\_mean}})\ *\ (\mbox{\hyperlink{_s_d_l__opengl_8h_a1675d9d7bb68e1657ff028643b4037e3}{y}}[i]\ -\/\ \mbox{\hyperlink{class_vector__2d}{y\_mean}});}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d}{den}}\ +=\ (\mbox{\hyperlink{_s_d_l__opengl_8h_abe1577120f1766eff913e9a74879f373}{x}}[i]\ -\/\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{x\_mean}})\ *\ (\mbox{\hyperlink{_s_d_l__opengl_8h_abe1577120f1766eff913e9a74879f373}{x}}[i]\ -\/\ \mbox{\hyperlink{class_vector__2d}{x\_mean}});}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \}}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d}{slope}}\ =\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_abb50fc1ead3a02a46fb52daa2045f95c}{num}}\ /\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{den}};}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d}{intercept}}\ =\ \mbox{\hyperlink{class_vector__2d}{y\_mean}}\ -\/\ \mbox{\hyperlink{class_vector__2d}{slope}}\ *\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{x\_mean}};}
\DoxyCodeLine{\ \ \ \ \}}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \textcolor{comment}{//\ Estimate\ parameters\ using\ gradient\ descent\ (Maximum\ Likelihood\ Estimation)}}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordtype}{void}\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{gradient\_descent}}(\textcolor{keyword}{const}\ std::vector<double>\&\ \mbox{\hyperlink{_s_d_l__opengl_8h_abe1577120f1766eff913e9a74879f373}{x}},\ \textcolor{keyword}{const}\ std::vector<double>\&\ \mbox{\hyperlink{_s_d_l__opengl_8h_a1675d9d7bb68e1657ff028643b4037e3}{y}},\ \textcolor{keywordtype}{double}\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_ac03904b1f7e359bb2acf10a755e97c5a}{alpha}}\ =\ 0.01,\ \textcolor{keywordtype}{int}\ \mbox{\hyperlink{class_vector__2d}{iterations}}\ =\ 1000)\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d}{slope}}\ =\ 0.0;}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d}{intercept}}\ =\ 0.0;}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}(\textcolor{keywordtype}{int}\ i\ =\ 0;\ i\ <\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{iterations}};\ i++)\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{double}\ \mbox{\hyperlink{class_vector__2d}{slope\_grad}}\ =\ 0.0;}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{double}\ \mbox{\hyperlink{class_vector__2d}{intercept\_grad}}\ =\ 0.0;}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordtype}{double}\ \mbox{\hyperlink{class_vector__2d}{N}}\ =\ \mbox{\hyperlink{_s_d_l__opengl_8h_abe1577120f1766eff913e9a74879f373}{x}}.size();}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}(\textcolor{keywordtype}{size\_t}\ \mbox{\hyperlink{class_vector__2d}{j}}\ =\ 0;\ \mbox{\hyperlink{class_vector__2d}{j}}\ <\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{N}};\ \mbox{\hyperlink{class_vector__2d}{j}}++)\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d}{slope\_grad}}\ +=\ -\/(2/\mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{N}})\ *\ \mbox{\hyperlink{_s_d_l__opengl_8h_abe1577120f1766eff913e9a74879f373}{x}}[\mbox{\hyperlink{class_vector__2d}{j}}]\ *\ (\mbox{\hyperlink{_s_d_l__opengl_8h_a1675d9d7bb68e1657ff028643b4037e3}{y}}[\mbox{\hyperlink{class_vector__2d}{j}}]\ -\/\ (\mbox{\hyperlink{class_vector__2d}{slope}}\ *\ \mbox{\hyperlink{_s_d_l__opengl_8h_abe1577120f1766eff913e9a74879f373}{x}}[\mbox{\hyperlink{class_vector__2d}{j}}]\ +\ \mbox{\hyperlink{class_vector__2d}{intercept}}));}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d}{intercept\_grad}}\ +=\ -\/(2/\mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{N}})\ *\ (\mbox{\hyperlink{_s_d_l__opengl_8h_a1675d9d7bb68e1657ff028643b4037e3}{y}}[\mbox{\hyperlink{class_vector__2d}{j}}]\ -\/\ (\mbox{\hyperlink{class_vector__2d}{slope}}\ *\ \mbox{\hyperlink{_s_d_l__opengl_8h_abe1577120f1766eff913e9a74879f373}{x}}[\mbox{\hyperlink{class_vector__2d}{j}}]\ +\ \mbox{\hyperlink{class_vector__2d}{intercept}}));}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ \}}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d}{slope}}\ =\ \mbox{\hyperlink{class_vector__2d}{slope}}\ -\/\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_ac03904b1f7e359bb2acf10a755e97c5a}{alpha}}\ *\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{slope\_grad}};}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d}{intercept}}\ =\ \mbox{\hyperlink{class_vector__2d}{intercept}}\ -\/\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_ac03904b1f7e359bb2acf10a755e97c5a}{alpha}}\ *\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{intercept\_grad}};}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \}}
\DoxyCodeLine{\ \ \ \ \}}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \textcolor{comment}{//\ Make\ prediction}}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordtype}{double}\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{predict}}(\textcolor{keywordtype}{double}\ \mbox{\hyperlink{_s_d_l__opengl_8h_abe1577120f1766eff913e9a74879f373}{x}})\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{class_vector__2d}{slope}}\ *\ \mbox{\hyperlink{_s_d_l__opengl_8h_abe1577120f1766eff913e9a74879f373}{x}}\ +\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{intercept}};}
\DoxyCodeLine{\ \ \ \ \}}
\DoxyCodeLine{\};}

\end{DoxyCode}



\begin{DoxyItemize}
\item The {\ttfamily least\+\_\+squares} method implements the analytical solution to the least squares problem for linear regression, while the {\ttfamily gradient\+\_\+descent} method implements a basic gradient descent algorithm for Maximum Likelihood Estimation (assuming normally distributed errors).
\item These methods will set the slope and intercept parameters of the class, which can then be used to make predictions with the {\ttfamily predict} method.
\item Note that this code does not include any error checking or handling, and the gradient descent method may not converge if the learning rate ({\ttfamily alpha}) is not properly set or the number of iterations is insufficient. The performance of this gradient descent implementation will also not be very good on large datasets, as it uses a simple loop to compute the gradient at each step. For larger datasets, a more efficient implementation would be required, possibly involving parallel computation.
\end{DoxyItemize}\hypertarget{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics_autotoc_md133}{}\doxysection{\texorpdfstring{A basic implementation of Linear Regression using Eigen.}{A basic implementation of Linear Regression using Eigen.}}\label{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics_autotoc_md133}
Note that this is a rudimentary implementation that doesn\textquotesingle{}t include features such as regularization or interaction terms, but it should give you a starting point for your own work.

This code doesn\textquotesingle{}t include the maximization of the likelihood function for MLE, as this is a bit more complex and depends heavily on the specificities of your problem.


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{preprocessor}{\#include\ <iostream>}}
\DoxyCodeLine{\textcolor{preprocessor}{\#include\ <Eigen/Dense>}}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{keyword}{class\ }\mbox{\hyperlink{class_vector__2d}{LinearRegression}}}
\DoxyCodeLine{\{}
\DoxyCodeLine{\textcolor{keyword}{public}:}
\DoxyCodeLine{\ \ \ \ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{LinearRegression}}()\ \{\}}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordtype}{void}\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{train}}(\textcolor{keyword}{const}\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::MatrixXd}}\ \&\mbox{\hyperlink{class_vector__2d}{X}},\ \textcolor{keyword}{const}\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::VectorXd}}\ \&\mbox{\hyperlink{_s_d_l__opengl_8h_a1675d9d7bb68e1657ff028643b4037e3}{y}})}
\DoxyCodeLine{\ \ \ \ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{comment}{//\ Add\ a\ column\ of\ ones\ for\ the\ bias\ term}}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::MatrixXd}}\ \mbox{\hyperlink{class_vector__2d}{Xb}}\ =\ Eigen::MatrixXd::Ones(\mbox{\hyperlink{class_vector__2d}{X}}.rows(),\ \mbox{\hyperlink{class_vector__2d}{X}}.cols()\ +\ 1);}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d}{Xb}}.block(0,\ 1,\ \mbox{\hyperlink{class_vector__2d}{X}}.rows(),\ \mbox{\hyperlink{class_vector__2d}{X}}.cols())\ =\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{X}};}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{comment}{//\ Compute\ the\ weights\ using\ the\ normal\ equation}}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_a3fc6a10c8f7ffa12a6a7855d06ff581d}{weights}}\ =\ (\mbox{\hyperlink{class_vector__2d}{Xb}}.transpose()\ *\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Xb}}).\mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{ldlt}}().solve(\mbox{\hyperlink{class_vector__2d}{Xb}}.transpose()\ *\ \mbox{\hyperlink{_s_d_l__opengl_8h_a1675d9d7bb68e1657ff028643b4037e3}{y}});}
\DoxyCodeLine{\ \ \ \ \}}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::VectorXd}}\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{predict}}(\textcolor{keyword}{const}\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::MatrixXd}}\ \&\mbox{\hyperlink{class_vector__2d}{X}})}
\DoxyCodeLine{\ \ \ \ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{comment}{//\ Add\ a\ column\ of\ ones\ for\ the\ bias\ term}}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::MatrixXd}}\ \mbox{\hyperlink{class_vector__2d}{Xb}}\ =\ Eigen::MatrixXd::Ones(\mbox{\hyperlink{class_vector__2d}{X}}.rows(),\ \mbox{\hyperlink{class_vector__2d}{X}}.cols()\ +\ 1);}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d}{Xb}}.block(0,\ 1,\ \mbox{\hyperlink{class_vector__2d}{X}}.rows(),\ \mbox{\hyperlink{class_vector__2d}{X}}.cols())\ =\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{X}};}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{class_vector__2d}{Xb}}\ *\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_a3fc6a10c8f7ffa12a6a7855d06ff581d}{weights}};}
\DoxyCodeLine{\ \ \ \ \}}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{keyword}{private}:}
\DoxyCodeLine{\ \ \ \ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::VectorXd}}\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_a3fc6a10c8f7ffa12a6a7855d06ff581d}{weights}};}
\DoxyCodeLine{\};}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{keywordtype}{int}\ \mbox{\hyperlink{_c_make_c_x_x_compiler_id_8cpp_a0ddf1224851353fc92bfbff6f499fa97}{main}}()}
\DoxyCodeLine{\{}
\DoxyCodeLine{\ \ \ \ \textcolor{comment}{//\ Create\ a\ simple\ dataset}}
\DoxyCodeLine{\ \ \ \ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::MatrixXd}}\ \mbox{\hyperlink{class_vector__2d}{X}}(100,\ 1);}
\DoxyCodeLine{\ \ \ \ \mbox{\hyperlink{class_vector__2d}{X}}\ <<\ Eigen::VectorXd::Random(100);}
\DoxyCodeLine{\ \ \ \ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::VectorXd}}\ \mbox{\hyperlink{_s_d_l__opengl_8h_a1675d9d7bb68e1657ff028643b4037e3}{y}}\ =\ 2.0\ *\ \mbox{\hyperlink{class_vector__2d}{X}}.array()\ +\ 0.5\ +\ Eigen::VectorXd::Random(100)\ /\ 10;}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \textcolor{comment}{//\ Train\ the\ model}}
\DoxyCodeLine{\ \ \ \ \mbox{\hyperlink{class_vector__2d}{LinearRegression}}\ \mbox{\hyperlink{class_vector__2d}{model}};}
\DoxyCodeLine{\ \ \ \ \mbox{\hyperlink{class_vector__2d}{model}}.train(\mbox{\hyperlink{class_vector__2d}{X}},\ \mbox{\hyperlink{_s_d_l__opengl_8h_a1675d9d7bb68e1657ff028643b4037e3}{y}});}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \textcolor{comment}{//\ Predict\ the\ output\ for\ the\ training\ data}}
\DoxyCodeLine{\ \ \ \ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::VectorXd}}\ \mbox{\hyperlink{class_vector__2d}{y\_pred}}\ =\ \mbox{\hyperlink{class_vector__2d}{model}}.predict(\mbox{\hyperlink{class_vector__2d}{X}});}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \textcolor{comment}{//\ Print\ the\ first\ 10\ predictions}}
\DoxyCodeLine{\ \ \ \ std::cout\ <<\ \mbox{\hyperlink{class_vector__2d}{y\_pred}}.head(10)\ <<\ std::endl;}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordflow}{return}\ 0;}
\DoxyCodeLine{\}}

\end{DoxyCode}



\begin{DoxyItemize}
\item This code first prepares the feature matrix X by adding a column of ones. This is done because the bias term (often called "{}intercept"{}) in linear regression can be included as an extra feature that is always set to one. The weights are then computed using the normal equation, which is a simple and efficient way to find the least squares solution for linear regression.
\item To implement linear regression with MLE, you will need to set up the likelihood function for the Gaussian distribution (which is the usual assumption for the error term in linear regression), and then use an optimization algorithm to find the parameters that maximize this function. This is more involved and requires the use of a nonlinear optimization library.
\item Please note that to run this program, you need to link the Eigen library. If you\textquotesingle{}re using CMake, add find\+\_\+package(\+Eigen3 REQUIRED) in your \doxylink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{CMake\+Lists.\+txt} file and link it with target\+\_\+link\+\_\+libraries(your\+\_\+target Eigen3\+::\+Eigen).
\end{DoxyItemize}\hypertarget{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics_autotoc_md134}{}\doxysection{\texorpdfstring{Maximum Likelihood Estimation}{Maximum Likelihood Estimation}}\label{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics_autotoc_md134}
Maximum likelihood estimation (MLE) in linear regression is a bit more complicated than the least squares method because it involves an optimization problem. You would typically use an optimization library to solve it.


\begin{DoxyItemize}
\item A key thing to understand about MLE in linear regression is that if the errors are normally distributed, then the maximum likelihood estimates for the regression coefficients are the same as the least squares estimates.
\item That said, here\textquotesingle{}s a basic example of a linear regression class where we use MLE for estimation assuming a normal distribution of errors. For simplicity, we will use the numerical optimization method provided by Eigen, Newton\textquotesingle{}s method\+:
\end{DoxyItemize}


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{preprocessor}{\#include\ <Eigen/Dense>}}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{keyword}{class\ }\mbox{\hyperlink{class_vector__2d}{LinearRegressionMLE}}}
\DoxyCodeLine{\{}
\DoxyCodeLine{\textcolor{keyword}{public}:}
\DoxyCodeLine{\ \ \ \ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{LinearRegressionMLE}}()\ \{\}}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordtype}{void}\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{train}}(\textcolor{keyword}{const}\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::MatrixXd}}\ \&\mbox{\hyperlink{class_vector__2d}{X}},\ \textcolor{keyword}{const}\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::VectorXd}}\ \&\mbox{\hyperlink{_s_d_l__opengl_8h_a1675d9d7bb68e1657ff028643b4037e3}{y}})}
\DoxyCodeLine{\ \ \ \ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{comment}{//\ Add\ a\ column\ of\ ones\ for\ the\ bias\ term}}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::MatrixXd}}\ \mbox{\hyperlink{class_vector__2d}{Xb}}\ =\ Eigen::MatrixXd::Ones(\mbox{\hyperlink{class_vector__2d}{X}}.rows(),\ \mbox{\hyperlink{class_vector__2d}{X}}.cols()\ +\ 1);}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d}{Xb}}.block(0,\ 1,\ \mbox{\hyperlink{class_vector__2d}{X}}.rows(),\ \mbox{\hyperlink{class_vector__2d}{X}}.cols())\ =\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{X}};}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{comment}{//\ initialize\ coefficients\ with\ Least\ Squares\ Estimation}}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d}{coefficients}}\ =\ (\mbox{\hyperlink{class_vector__2d}{Xb}}.transpose()\ *\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Xb}}).\mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{ldlt}}().solve(\mbox{\hyperlink{class_vector__2d}{Xb}}.transpose()\ *\ \mbox{\hyperlink{_s_d_l__opengl_8h_a1675d9d7bb68e1657ff028643b4037e3}{y}});}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{comment}{//\ use\ newton's\ method\ to\ find\ coefficients\ that\ maximize\ log-\/likelihood}}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ i\ =\ 0;\ i\ <\ 100;\ ++i)\ \textcolor{comment}{//\ arbitrary\ number\ of\ steps}}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::VectorXd}}\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_afa0fb1b5e976920c0abeff2dca3ed774}{h}}\ =\ \mbox{\hyperlink{class_vector__2d}{Xb}}\ *\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{coefficients}};}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::VectorXd}}\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_aa5367c14d90f462230c2611b81b41d23}{p}}\ =\ 1\ /\ (1\ +\ (-\/\mbox{\hyperlink{_s_d_l__opengl__glext_8h_afa0fb1b5e976920c0abeff2dca3ed774}{h}}.array()).\mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{exp}}());\ \textcolor{comment}{//\ logistic\ function}}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::VectorXd}}\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_afb1b07e1b25035d41d60fb2c03d507e6}{w}}\ =\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_aa5367c14d90f462230c2611b81b41d23}{p}}.array()\ *\ (1\ -\/\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_aa5367c14d90f462230c2611b81b41d23}{p}}.array());}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::VectorXd}}\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_a1c599441d9dece861ee2cd70e31ce120}{z}}\ =\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_afa0fb1b5e976920c0abeff2dca3ed774}{h}}\ +\ (\mbox{\hyperlink{_s_d_l__opengl_8h_a1675d9d7bb68e1657ff028643b4037e3}{y}}\ -\/\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_aa5367c14d90f462230c2611b81b41d23}{p}}).\mbox{\hyperlink{_s_d_l__opengl__glext_8h_a52f38e7d822a46377fde7a02708eedb1}{array}}()\ /\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_afb1b07e1b25035d41d60fb2c03d507e6}{w}}.array();}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d}{coefficients}}\ =\ (\mbox{\hyperlink{class_vector__2d}{Xb}}.transpose()\ *\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_afb1b07e1b25035d41d60fb2c03d507e6}{w}}.asDiagonal()\ *\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Xb}}).\mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{ldlt}}().solve(\mbox{\hyperlink{class_vector__2d}{Xb}}.transpose()\ *\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_afb1b07e1b25035d41d60fb2c03d507e6}{w}}.asDiagonal()\ *\ \mbox{\hyperlink{_s_d_l__opengl__glext_8h_a1c599441d9dece861ee2cd70e31ce120}{z}});}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \}}
\DoxyCodeLine{\ \ \ \ \}}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::VectorXd}}\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{predict}}(\textcolor{keyword}{const}\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::MatrixXd}}\ \&\mbox{\hyperlink{class_vector__2d}{X}})}
\DoxyCodeLine{\ \ \ \ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{comment}{//\ Add\ a\ column\ of\ ones\ for\ the\ bias\ term}}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::MatrixXd}}\ \mbox{\hyperlink{class_vector__2d}{Xb}}\ =\ Eigen::MatrixXd::Ones(\mbox{\hyperlink{class_vector__2d}{X}}.rows(),\ \mbox{\hyperlink{class_vector__2d}{X}}.cols()\ +\ 1);}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \mbox{\hyperlink{class_vector__2d}{Xb}}.block(0,\ 1,\ \mbox{\hyperlink{class_vector__2d}{X}}.rows(),\ \mbox{\hyperlink{class_vector__2d}{X}}.cols())\ =\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{X}};}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ \mbox{\hyperlink{class_vector__2d}{Xb}}\ *\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{coefficients}};}
\DoxyCodeLine{\ \ \ \ \}}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{keyword}{private}:}
\DoxyCodeLine{\ \ \ \ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{Eigen::VectorXd}}\ \mbox{\hyperlink{class_vector__2d_a6d09fb691e01f179efcbef03bef13b9c}{coefficients}};}
\DoxyCodeLine{\};}

\end{DoxyCode}


This example sets up a logistic regression problem and uses Newton\textquotesingle{}s method to find the maximum likelihood estimate of the parameters. Please note that this implementation is a simplification and doesn\textquotesingle{}t include many things that you would typically want in a machine learning algorithm, such as regularization, error checking, or a stopping criterion for the optimization process. It is provided as a simple starting point for your own work.\hypertarget{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics_autotoc_md135}{}\doxysection{\texorpdfstring{Optimization Techniques to Estimate The Maximum Likelihood Parameters}{Optimization Techniques to Estimate The Maximum Likelihood Parameters}}\label{md_docs_2own__docs_2cpp__fudamentals_2_math__and___statistics_autotoc_md135}
There are several optimization techniques that can be used to estimate the parameters in a maximum likelihood model. These techniques are designed to find the parameters that maximize the likelihood function, often iteratively improving the estimates. Here are a few key methods\+:


\begin{DoxyEnumerate}
\item {\bfseries{Gradient Descent\+:}} This is a first-\/order iterative optimization algorithm for finding the minimum of a function. In the context of maximum likelihood estimation (MLE), we would find the maximum by using the negative of the function (or equivalently, minimize the negative log-\/likelihood). It is based on the observation that if a multivariable function is defined and differentiable in a neighborhood of a point, then the function decreases fastest if one goes in the direction of the negative gradient.
\item {\bfseries{Stochastic Gradient Descent (SGD)\+:}} It\textquotesingle{}s a version of gradient descent, but instead of calculating the gradient on the entire dataset, it estimates the gradient on a subset (batch) of data. SGD is useful when dealing with large datasets.
\item {\bfseries{Newton\textquotesingle{}s Method (also called the Newton-\/\+Raphson method)\+:}} This method uses the second-\/order Taylor series expansion to approximate the objective function, leading to faster convergence compared to gradient descent. However, this requires calculation of the second derivative (Hessian matrix), which can be computationally intensive for high-\/dimensional data.
\item {\bfseries{Quasi-\/\+Newton Methods (BFGS, L-\/\+BFGS)\+:}} These methods also approximate the objective function using the second-\/order Taylor series expansion, but they update an estimate of the Hessian matrix iteratively, which is less computationally intensive than directly calculating the Hessian matrix.
\item {\bfseries{Expectation-\/\+Maximization (EM) Algorithm\+:}} This is particularly useful when the model depends on some hidden or latent variables. The EM algorithm iteratively performs an expectation (E) step, which creates a function for the expectation of the log-\/likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-\/likelihood found on the E step.
\item {\bfseries{Conjugate Gradient Method\+:}} This is an algorithm for the numerical solution of particular systems of linear equations, namely those whose matrix is symmetric and positive-\/definite. The conjugate gradient method is often implemented as an iterative algorithm, applicable to sparse systems that are too large to be handled by a direct implementation or other direct methods such as the Cholesky decomposition.
\end{DoxyEnumerate}

It should be noted that the best method to use for a given problem depends on many factors, including the structure of the likelihood function, the number of parameters, the size of the dataset, and the specifics of the computing environment. 